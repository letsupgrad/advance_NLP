import streamlit as st
import pandas as pd
import nltk
from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.corpus import stopwords
import pdfplumber
import docx
import requests
from bs4 import BeautifulSoup
from wordcloud import WordCloud
import matplotlib.pyplot as plt
import spacy
from transformers import pipeline
from langdetect import detect
from googletrans import Translator
import whisper
import tempfile

# CRITICAL gensim import - no change here
from gensim import corpora, models
import pyLDAvis.gensim_models as gensimvis
import pyLDAvis

import streamlit.components.v1 as components

# Initialize downloads for nltk if needed
nltk.download("punkt")
nltk.download("stopwords")

# --- Load models with caching ---
@st.cache_resource
def load_models():
    try:
        summarizer = pipeline("summarization", model="t5-small", tokenizer="t5-small")
        # Download spacy model if not present
        try:
            nlp = spacy.load("en_core_web_sm")
        except:
            st.info("Downloading Spacy model...")
            spacy.cli.download("en_core_web_sm")
            nlp = spacy.load("en_core_web_sm")
        
        whisper_model = whisper.load_model("base")
        return summarizer, nlp, whisper_model
    except Exception as e:
        st.error(f"Error loading models: {e}")
        # Return placeholders to prevent app from crashing
        return None, None, None

summarizer, ner_model, whisper_model = load_models()

try:
    translator = Translator()
except:
    st.warning("Translation service unavailable")
    translator = None

# --- Utility functions ---
def extract_text_from_file(file, filetype):
    if filetype == "txt":
        return file.read().decode("utf-8")
    elif filetype == "pdf":
        with pdfplumber.open(file) as pdf:
            return "\n".join(page.extract_text() for page in pdf.pages if page.extract_text())
    elif filetype == "docx":
        doc = docx.Document(file)
        return "\n".join([p.text for p in doc.paragraphs])
    elif filetype == "csv":
        df = pd.read_csv(file)
        return df.to_string()
    else:
        return ""

def extract_text_from_url(url):
    try:
        response = requests.get(url, timeout=10)
        soup = BeautifulSoup(response.text, "html.parser")
        paragraphs = soup.find_all("p")
        return " ".join(p.text for p in paragraphs)
    except Exception as e:
        return f"Error fetching URL: {e}"

def summarize_text(text, num_sentences=5):
    sentences = sent_tokenize(text)
    words = word_tokenize(text.lower())
    stop_words = set(stopwords.words("english"))
    filtered_words = [w for w in words if w.isalnum() and w not in stop_words]

    freq = pd.Series(filtered_words).value_counts()
    sentence_scores = {}
    for sent in sentences:
        score = sum(freq.get(word.lower(), 0) for word in word_tokenize(sent))
        sentence_scores[sent] = score

    ranked = sorted(sentence_scores, key=sentence_scores.get, reverse=True)
    summary = " ".join(ranked[:num_sentences])
    return summary

def get_keywords(text, top_n=20):
    words = word_tokenize(text.lower())
    words = [w for w in words if w.isalnum()]
    stop_words = set(stopwords.words("english"))
    filtered = [w for w in words if w not in stop_words]
    freq = pd.Series(filtered).value_counts().reset_index()
    freq.columns = ["Keyword", "Count"]
    return freq.head(top_n)

def plot_wordcloud(freq_df):
    word_freq = dict(zip(freq_df["Keyword"], freq_df["Count"]))
    wc = WordCloud(width=800, height=300, background_color="white").generate_from_frequencies(word_freq)
    fig, ax = plt.subplots()
    ax.imshow(wc, interpolation="bilinear")
    ax.axis("off")
    st.pyplot(fig)

def translate_to_english(text):
    try:
        if translator:
            detected_lang = detect(text)
            if detected_lang != "en":
                translated = translator.translate(text, src=detected_lang, dest="en")
                return translated.text, detected_lang
        return text, "en"
    except:
        return text, "en"

def translate_back(text, lang):
    try:
        if translator and lang != "en":
            return translator.translate(text, src="en", dest=lang).text
        return text
    except:
        return text

def transcribe_audio(audio_file):
    if whisper_model:
        with tempfile.NamedTemporaryFile(delete=False, suffix='.tmp') as tmp:
            tmp.write(audio_file.read())
            tmp_path = tmp.name
        
        try:
            result = whisper_model.transcribe(tmp_path)
            return result["text"]
        except Exception as e:
            return f"Error transcribing audio: {e}"
    else:
        return "Whisper model not available"

def show_lda(text):
    try:
        words = [word for word in word_tokenize(text.lower()) if word.isalnum() and word not in stopwords.words("english")]
        dictionary = corpora.Dictionary([words])
        corpus = [dictionary.doc2bow(words)]
        lda_model = models.LdaModel(corpus, num_topics=2, id2word=dictionary, passes=10)
        vis = gensimvis.prepare(lda_model, corpus, dictionary)
        html = pyLDAvis.prepared_data_to_html(vis)
        components.html(html, width=1000, height=600)
    except Exception as e:
        st.error(f"Error generating LDA visualization: {e}")

# --- Streamlit UI ---
st.set_page_config(page_title="Ultimate NLP Summarizer", layout="wide")
st.title("üß† Ultimate AI Text Summarizer & Keyword Extractor")

# Sidebar
st.sidebar.header("Input & Settings")
input_method = st.sidebar.radio("Choose Input Method:", ["üìÅ Upload File", "üîó Enter URL", "üé§ Upload Audio"])
num_sentences = st.sidebar.slider("Summary Length (sentences)", 1, 15, 5)
summarization_type = st.sidebar.radio("Summarization Type:", ["Extractive (TF-IDF)", "Abstractive (T5)"])
show_topics = st.sidebar.checkbox("Show Topics (LDA)", value=False)

text = ""
filename = None

# Input Section
if input_method == "üìÅ Upload File":
    uploaded_file = st.file_uploader("Upload a file (txt, pdf, docx, csv)", type=["txt", "pdf", "docx", "csv"])
    if uploaded_file:
        filetype = uploaded_file.name.split(".")[-1].lower()
        filename = uploaded_file.name
        text = extract_text_from_file(uploaded_file, filetype)

elif input_method == "üîó Enter URL":
    url = st.text_input("Enter URL:")
    if url:
        text = extract_text_from_url(url)
        filename = "webpage"

elif input_method == "üé§ Upload Audio":
    audio_file = st.file_uploader("Upload audio (mp3, wav)", type=["mp3", "wav"])
    if audio_file:
        st.info("Transcribing audio...")
        text = transcribe_audio(audio_file)
        filename = "audio_transcript"
        st.write("üìù Transcript:")
        st.write(text)

# Preview
if text:
    st.subheader("üëÄ Preview of Input Text (first 1000 chars)")
    st.text(text[:1000] + ("..." if len(text) > 1000 else ""))

    # Translate if needed
    text_en, orig_lang = translate_to_english(text)

    # Summarize
    if st.button("üîç Generate Summary"):
        with st.spinner("Summarizing..."):
            if summarizer and summarization_type == "Abstractive (T5)":
                short_text = text_en[:1000]
                try:
                    result = summarizer(short_text, max_length=150, min_length=40, do_sample=False)
                    summary = result[0]["summary_text"]
                except Exception as e:
                    st.error(f"Error with abstractive summarization: {e}")
                    summary = summarize_text(text_en, num_sentences)
            else:
                summary = summarize_text(text_en, num_sentences)

            summary = translate_back(summary, orig_lang)

        st.subheader("üß† Summary")
        st.success(summary)

        # Download button
        st.download_button(
            label="üì• Download Summary",
            data=summary,
            file_name=f"{filename}_summary.txt" if filename else "summary.txt",
            mime="text/plain",
        )

        # Keywords and Wordcloud
        st.subheader("üîë Keywords")
        keywords = get_keywords(text_en)
        col1, col2 = st.columns(2)
        with col1:
            st.table(keywords)
        with col2:
            plot_wordcloud(keywords)

        # Named Entities
        if ner_model:
            st.subheader("üß¨ Named Entities")
            try:
                doc = ner_model(text_en[:2000])
                entities = [(ent.text, ent.label_) for ent in doc.ents]
                if entities:
                    df_entities = pd.DataFrame(entities, columns=["Entity", "Type"])
                    st.table(df_entities)
                else:
                    st.info("No named entities found.")
            except Exception as e:
                st.error(f"Error extracting entities: {e}")

        # Topic modeling
        if show_topics:
            st.subheader("üìä Topic Modeling (LDA)")
            show_lda(text_en)

else:
    st.info("Upload a file, enter a URL, or upload audio to get started!")
